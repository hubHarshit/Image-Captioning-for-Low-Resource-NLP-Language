Low resource languages are languages that have limited or insufficient data available for natural language processing (NLP) tasks. These languages often have small populations of speakers, and may not have a long history of written texts or a developed infrastructure for collecting and digitizing linguistic data.
Some examples of low resource languages include:
Indigenous languages: Many indigenous languages have small numbers of speakers and may not have a long history of written texts. Examples include Hawaiian, Maori, and Yucatec Maya.
Less widely spoken languages: There are many languages that are spoken by relatively small numbers of people, but are not necessarily indigenous languages. Examples include Basque, Estonian, and Welsh.
Low-income or developing countries: In some cases, low resource languages may be spoken in low-income or developing countries where there may be less infrastructure or funding available for language technology research and development.
Developing NLP tools and resources for low resource languages can be challenging, but it is important for preserving linguistic diversity and enabling equal access to information and communication. Researchers and organizations are working on developing techniques and resources to support the development of NLP tools for low resource languages.

Image captioning is the process of generating a textual description of an image or video. It involves using machine learning algorithms to analyze the content of an image and generate a natural language description of it. Image captioning can be useful for a variety of applications, including helping visually impaired people understand the content of images and videos, and improving the accessibility of visual media.There are several approaches to building image captioning systems. One common approach involves using a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs). The CNN is used to extract features from the image, and the RNN is used to generate the natural language description based on the features extracted by the CNN.
Other approaches to image captioning involve using transformers or other types of neural networks. In general, the goal of image captioning is to produce descriptive and accurate captions that accurately describe the content of the image.

This project entailed generating captions for images in three different languages- Hausa( a chadic language), Kyrgyz( native anguage for Kyrgyzthan)  and Thai( native language of Thailand). 
The project takes three different approaches for doing so. 
Approach 1- Thai Language
Comp


